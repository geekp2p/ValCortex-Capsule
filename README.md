# ValCortex + Gemma-2 Setup (Windows)

## -0.1 ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á
```powershell
New-Item -ItemType Directory -Force -Path G:\models | Out-Null
```

# Gemma-2 2B Instruct (GGUF, Q4_K_M)
```powershell
curl.exe -L "https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it-Q4_K_M.gguf?download=true" -o "G:\models\gemma-2-2b-it-Q4_K_M.gguf"
```


# ValCortex + vicuna-13b Setup (Windows)

## 0. ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• (.gguf)

‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡πâ‡∏ß‡∏¢ quant Q4_K_M (‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û/‡πÅ‡∏£‡∏°)

# ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á
```powershell
New-Item -ItemType Directory -Force -Path G:\models | Out-Null
```

# ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (Q4_K_M ~7.9GB)
```powershell
curl.exe -L "https://huggingface.co/cjpais/llava-v1.6-vicuna-13b-gguf/resolve/main/llava-v1.6-vicuna-13b.Q4_K_M.gguf?download=true" -o "G:\models\llava-v1.6-vicuna-13b.Q4_K_M.gguf"
```


# ValCortex + GPT-OSS-20B Setup (Windows)

## 1. ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• (.gguf)

> ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà `G:\models`

```powershell
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÄ‡∏Å‡πá‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ)
New-Item -ItemType Directory -Force -Path G:\models | Out-Null
```

```powershell
# ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå quant ‡πÅ‡∏ö‡∏ö Q4_K_M (‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡πÅ‡∏£‡∏° ~16GB+)
curl.exe -L "https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-Q4_K_M.gguf?download=true" -o "G:\models\gpt-oss-20b-Q4_K_M.gguf"
```

üìå ‡∏ó‡∏µ‡πà‡∏°‡∏≤ Hugging Face: [unsloth/gpt-oss-20b-GGUF](https://huggingface.co/unsloth/gpt-oss-20b-GGUF)  
(‡πÄ‡∏•‡∏∑‡∏≠‡∏Å quant ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡πÑ‡∏î‡πâ ‡πÄ‡∏ä‡πà‡∏ô Q5_K_M, Q8_0 ‡∏ï‡∏≤‡∏°‡∏™‡πÄ‡∏õ‡∏Å‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á)

---




Gemma2B Single-EXE Capsule
==========================
- ‡πÅ‡∏Ñ‡πà‡∏£‡∏±‡∏ô EXE ‡∏Å‡πá‡∏û‡∏≠: ‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏à‡∏∞‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Ollama ‡πÄ‡∏≠‡∏á (zip), ‡πÅ‡∏ï‡∏Å‡πÑ‡∏õ‡∏ó‡∏µ‡πà .\bin\, ‡πÅ‡∏•‡πâ‡∏ß‡∏™‡∏£‡πâ‡∏≤‡∏á Modelfile ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
- Modelfile ‡∏ï‡∏±‡πâ‡∏á‡∏ï‡πâ‡∏ô‡∏ä‡∏µ‡πâ‡πÑ‡∏õ‡∏ó‡∏µ‡πà G:\models\gemma-2-2b-it-Q4_K_M.gguf (‡πÅ‡∏Å‡πâ‡πÑ‡∏ü‡∏•‡πå Modelfile ‡πÑ‡∏î‡πâ‡∏ñ‡πâ‡∏≤‡∏û‡∏≤‡∏ò‡∏Ñ‡∏∏‡∏ì‡∏ï‡πà‡∏≤‡∏á)
- ‡πÄ‡∏õ‡∏¥‡∏î REST API: http://127.0.0.1:8088  (POST /chat)

‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ
1) ‡πÅ‡∏ï‡∏Å zip ‡πÑ‡∏õ‡∏ó‡∏µ‡πà G:\ai\gemma2b_singleexe\
2) PowerShell:
   cd G:\ai\gemma2b_singleexe
   .\build.ps1
   .\gemma2b.exe
3) ‡∏ó‡∏î‡∏™‡∏≠‡∏ö:
   curl -X POST "http://127.0.0.1:8088/chat" -H "Content-Type: application/json" -d "{`"prompt`":`"‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ï‡∏±‡∏ß 3 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î`"}"



curl.exe -s "http://127.0.0.1:8088/models"

curl.exe -s -X POST "http://127.0.0.1:8088/select" -H "Content-Type: application/json" -d "{\"tag\":\"gguf-gpt-oss-20b-q4_k_m\"}"

curl.exe -s -X POST "http://127.0.0.1:8088/chat" -H "Content-Type: application/json" -d "{\"prompt\":\"‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á 3 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î\"}"

curl.exe -s -X POST "http://127.0.0.1:8088/chat" -H "Content-Type: application/json" -d "{\"model\":\"gguf-gemma-2-2b-it-q4_k_m\",\"prompt\":\"‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì\"}"

curl.exe -s -X POST "http://127.0.0.1:8088/chat" -H "Content-Type: application/json" -d "{\"model\":\"gguf-llava-v1-6-vicuna-13b-q4_k_m\",\"prompt\":\"‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏±‡πâ‡∏ô ‡πÜ ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á\"}"

curl.exe -s -X POST "http://127.0.0.1:11434/api/chat" -H "Content-Type: application/json" -d "{\"model\":\"gguf-llava-v1-6-vicuna-13b-q4_k_m\",\"messages\":[{\"role\":\"user\",\"content\":\"‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡∏†‡∏≤‡∏û‡∏ô‡∏µ‡πâ\",\"images\":[\"BASE64_IMAGE_HERE\"]}]}"
